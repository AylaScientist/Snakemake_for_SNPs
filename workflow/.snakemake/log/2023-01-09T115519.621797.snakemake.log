Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
all                     1              1              1
merge_dataframes        1             12             12
total                   2              1             12

Select jobs to execute...

[Mon Jan  9 11:55:20 2023]
rule merge_dataframes:
    input: workflow/AD_GT_counts_bi_GF6.csv, workflow/AD_GT_counts_bi_KS4.csv
    output: workflow/merged_df.csv
    log: logs/workflow/merge_dataframes.log
    jobid: 1
    reason: Missing output files: workflow/merged_df.csv
    threads: 12
    resources: tmpdir=/tmp, mem_mb=12000

[Mon Jan  9 11:55:20 2023]
Error in rule merge_dataframes:
    jobid: 1
    input: workflow/AD_GT_counts_bi_GF6.csv, workflow/AD_GT_counts_bi_KS4.csv
    output: workflow/merged_df.csv
    log: logs/workflow/merge_dataframes.log (check log file(s) for error message)
    conda-env: /home/fish/Snakemake_for_SNPs/workflow/.snakemake/conda/06703ee70a1e765ec642f5b57953594f_

RuleException:
CalledProcessError in line 17 of /home/fish/Snakemake_for_SNPs/workflow/rules/workflow.smk:
Command 'set -euo pipefail;  /home/fish/anaconda3/envs/pipeline/bin/python /home/fish/Snakemake_for_SNPs/workflow/.snakemake/scripts/tmpvxaakm0f.mergedataframes.py' returned non-zero exit status 1.
  File "/home/fish/Snakemake_for_SNPs/workflow/rules/workflow.smk", line 17, in __rule_merge_dataframes
  File "/home/fish/anaconda3/envs/pipeline/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2023-01-09T115519.621797.snakemake.log
